
# RAG-based Document Q&A System

This project is a **full-stack Retrieval-Augmented Generation (RAG)** system for answering questions from uploaded PDF documents. It uses:

- **Next.js (React)** for the frontend
- **FastAPI** for the backend API
- **LangChain + ChromaDB** for RAG pipeline
- **google/flan-t5-base** as the LLM
- **all-MiniLM-L6-v2** for embeddings

---

## ğŸ“ Project Structure

```
full-stack-QA-application/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app with PDF upload and chat API
â”‚   â”œâ”€â”€ config.py            # Model and DB config
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py       # Pydantic schema
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ pdf_processor.py # PDF file handling
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py  # LLM + Retriever QA logic
â”‚   â”‚   â””â”€â”€ vector_store.py  # Embedding & vector DB logic
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ pages/               # Next.js pages
â”‚   â”œâ”€â”€ components/          # FileUpload & ChatInterface
â”‚   â”œâ”€â”€ styles/              # Basic styling
â”‚   â”œâ”€â”€ package.json         # NPM config
â”‚   â””â”€â”€ tsconfig.json        # TypeScript config
â””â”€â”€ data/                    # Uploaded PDFs
```

---

## ğŸš€ 1. Backend Setup (FastAPI + LangChain + HuggingFace)

### ğŸ”§ Prerequisites

- Python 3.10+
- pip or conda
- Optional: virtualenv

---

## ğŸ” Environment Variables

Before running the backend, create a `.env` file inside the `backend/` folder with the following content:

```env
GOOGLE_API_KEY=add_your_key_here
VECTOR_DB_PATH=./vector_store
PDF_UPLOAD_PATH=../data

---

### ğŸ“¦ Install

```bash
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### â–¶ï¸ Run the backend

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

API Docs: http://localhost:8000/docs

---

## ğŸ’» 2. Frontend Setup (Next.js + React)

### ğŸ”§ Prerequisites

- Node.js 16+
- npm or yarn

### ğŸ“¦ Install

```bash
cd frontend
npm install
```

### â–¶ï¸ Run the frontend

```bash
npm run dev
```

Visit: http://localhost:3000

---

## ğŸ§  3. RAG Model Pipeline

### âœ¨ Embedding Model
- `sentence-transformers/all-MiniLM-L6-v2`
- Loaded using LangChain HuggingFaceEmbeddings

### ğŸ§  LLM Model
- `google/flan-t5-base` via Transformers pipeline
- Used with LangChain's `HuggingFacePipeline`

### ğŸ“š Vector DB
- `ChromaDB`
- Stores PDF chunks and enables semantic search

---

## âš™ï¸ 4. Backend Endpoints

| Method | Endpoint        | Description                         |
|--------|------------------|-------------------------------------|
| POST   | /api/upload      | Upload and chunk a PDF              |
| GET    | /api/documents   | List all uploaded documents         |
| POST   | /api/chat        | Ask a question, get RAG answer      |

---

## ğŸ§ª Example Workflow

1. Start backend and frontend
2. Go to http://localhost:3000
3. Upload a PDF file
4. Ask a question (e.g., â€œWhat is the total revenue in 2025?â€)
5. Get an answer with referenced source chunks

---

## ğŸ“Œ Notes

- PDF files are stored in `backend/data/`
- Vector DB is saved in `backend/chroma_db/`
- Embedding and LLM models are loaded via HuggingFace
- For arithmetic answers (e.g., debt ratio), the backend can be extended with logic

---

## ğŸ§¾ Example Output Snapshot

Hereâ€™s an example when asking:

**Question:** What is the total revenue for 2025?

**Answer:**
```
Sesuai dengan ketentuan dalam Perjanjian Pinjaman Sindikasi, 31 Maret 2025, LSA dan SCM melakukan penarikan masing-masing sebesar AS$27.000 dan AS$13.000 dan melakukan pembayaran masing-masing sebesar AS$27.000 dan AS$13.000 atas fasilitas pinjaman ini (untuk periode tiga bulan yang berakhir pada tanggal 31 Maret 2024: AS$nihil). Pada tanggal 31 Maret 2025 dan 31 Desember 2024, saldo pinjaman yang terutang dari fasilitas pinjaman ini adalah sebesar AS$nihil.
```

**Sources:**
- data\856384710-FinancialStatement-2025-I-AADI.pdf


Below is a visual example of the PDF Q&A result in the browser:

![Chat Result Example](result.png)

